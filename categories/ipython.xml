<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>绿萝间 (ipython)</title><link>https://muxuezi.github.io/</link><description></description><atom:link rel="self" type="application/rss+xml" href="https://muxuezi.github.io/categories/ipython.xml"></atom:link><language>en</language><lastBuildDate>Sat, 11 Jun 2016 02:25:33 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>SQLAlchemy Introduce</title><link>https://muxuezi.github.io/posts/sqlalchemy-introduce.html</link><dc:creator>Tao Junjie</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="perface"&gt;perface&lt;a class="anchor-link" href="https://muxuezi.github.io/posts/sqlalchemy-introduce.html#perface"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;我们每天都要面对数据，数据库CRUD操作的能力对每个任务都至关重要。无论你是开发网页，桌面或其他应用，他们都需要快速且安全的接入数据。关系型数据库仍然是储存数据最主流的手段之一。&lt;/p&gt;
&lt;p&gt;SQL是操作数据的利器，不过有时候要整合到应用里非常麻烦。作为一个Pythoner，你可能需要通过ODBC接口或者数据库API，用一些字符串生成查询。虽然这些方法可以有效的处理数据，但是数据安全和修改变得很困难。&lt;/p&gt;
&lt;p&gt;这本书将介绍一个非常强大且具有灵活性的Python库SQLAlchemy，可以跨越关系型数据库与传统编程直接的鸿沟。SQLAlchemy允许你使用原始的SQL语言查询，更鼓励使用更高级的Pythonic展示方式和更友好的方法实现数据的查询和更新。它提供工具让你一次性把应用的类和对象映射成数据库的表，然后就可以不再理会表结构了，还可以让你不断优化模型的性能。&lt;/p&gt;
&lt;p&gt;SQLAlchemy功能强大且充满柔性，但它还是有点复杂。SQLAlchemy教程只介绍了它的冰山一角，尽管在线文档也很全，但是更适合做参考书，不适合做教程。这本书的目的是要成为一个学习工具和一个方便的参考，帮你快速完成任务。&lt;/p&gt;
&lt;p&gt;本书内容用的SQLAlchemy是1.0版本；但是之前的版本都可以用。0.8版可能需要一点小改动，大部分都是源自0.5版。&lt;/p&gt;
&lt;p&gt;这本书分成三部分：SQLAlchemy Core，SQLAlchemy ORM和一个Cookbook部分。前两部分尽可能的做到一致性。我们有意在两个部分用同意的例子，目的是让你更方便的比较两种方式的用法。因此，你也可以根据自己的需要阅读本书SQLAlchemy Core和SQLAlchemy ORM的章节。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/sqlalchemy-introduce.html"&gt;Read more…&lt;/a&gt; (15 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>ipython</category><category>sql</category><guid>https://muxuezi.github.io/posts/sqlalchemy-introduce.html</guid><pubDate>Sun, 13 Dec 2015 11:17:02 GMT</pubDate></item><item><title>Postgres Import CSV cp936 Error</title><link>https://muxuezi.github.io/posts/postgres-import-csv-cp936-error.html</link><dc:creator>Tao Junjie</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Postgres导入CSV中文编码错误"&gt;Postgres导入CSV中文编码错误&lt;a class="anchor-link" href="https://muxuezi.github.io/posts/postgres-import-csv-cp936-error.html#Postgres%E5%AF%BC%E5%85%A5CSV%E4%B8%AD%E6%96%87%E7%BC%96%E7%A0%81%E9%94%99%E8%AF%AF"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;现在公司数据资源非常丰富，动辄要处理的csv就1E8行纪录，10G，分析起来相当爽。编码格式是cp936，用官方的COPY命令导入Postgres，一直出错，目前COPY的ENCODING不支持cp936。试着用Pandas读几行，保持csv为utf-8再COPY没问题。&lt;/p&gt;
&lt;p&gt;解决办法：通过Python3的Pandas读取CSV转换成utf－8，再利用sqlalchemy＋psycopg2（目前只支持到Python3.4，还不支持3.5）导入即可。&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;这么多依赖推荐使用&lt;a href="https://www.continuum.io/downloads"&gt;Anaconda&lt;/a&gt;，十分方便，尤其适合解决在Windows平台编译C语言相关库（lxml，psycopg2，numpy，pandas）时的各种奇葩问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;具体方法如下：&lt;/p&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/postgres-import-csv-cp936-error.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>ipython</category><category>sql</category><guid>https://muxuezi.github.io/posts/postgres-import-csv-cp936-error.html</guid><pubDate>Sun, 13 Dec 2015 11:11:05 GMT</pubDate></item><item><title>2-working-with-linear-models</title><link>https://muxuezi.github.io/posts/2-working-with-linear-models.html</link><dc:creator>Tao Junjie</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="处理线性模型"&gt;处理线性模型&lt;a class="anchor-link" href="https://muxuezi.github.io/posts/2-working-with-linear-models.html#%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;本章包括以下主题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://muxuezi.github.io/posts/fitting-a-line-through-data.html"&gt;线性回归模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://muxuezi.github.io/posts/evaluating-the-linear-regression-model.html"&gt;评估线性回归模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://muxuezi.github.io/posts/using-ridge-regression-to-overcome-linear-regression-shortfalls.html"&gt;用岭回归弥补线性回归的不足&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://muxuezi.github.io/posts/optimizing-the-ridge-regression-parameter.html"&gt;优化岭回归参数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://muxuezi.github.io/posts/using-sparsity-to-regularize-models.html"&gt;LASSO正则化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://muxuezi.github.io/posts/taking-a-more-fundamental-approach-to-regularization-with-lars.html"&gt;LARS正则化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://muxuezi.github.io/posts/using-linear-methods-for-classification-logistic-regression.html"&gt;用线性方法处理分类问题——逻辑回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://muxuezi.github.io/posts/directly-applying-bayesian-ridge-regression.html"&gt;贝叶斯岭回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://muxuezi.github.io/posts/using-boosting-to-learn-from-errors.html"&gt;用梯度提升回归从误差中学习&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/2-working-with-linear-models.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><category>scikit-learn cookbook</category><guid>https://muxuezi.github.io/posts/2-working-with-linear-models.html</guid><pubDate>Tue, 18 Aug 2015 05:07:14 GMT</pubDate></item><item><title>using-linear-methods-for-classification-logistic-regression</title><link>https://muxuezi.github.io/posts/using-linear-methods-for-classification-logistic-regression.html</link><dc:creator>Tao Junjie</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="用线性方法处理分类问题——逻辑回归"&gt;用线性方法处理分类问题——逻辑回归&lt;a class="anchor-link" href="https://muxuezi.github.io/posts/using-linear-methods-for-classification-logistic-regression.html#%E7%94%A8%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;实际上线性模型也可以用于分类任务。方法是把一个线性模型拟合成某个类型的概率分布，然后用一个函数建立阈值来确定结果属于哪一类。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/using-linear-methods-for-classification-logistic-regression.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><category>scikit-learn cookbook</category><guid>https://muxuezi.github.io/posts/using-linear-methods-for-classification-logistic-regression.html</guid><pubDate>Tue, 18 Aug 2015 04:57:47 GMT</pubDate></item><item><title>optimizing-the-ridge-regression-parameter</title><link>https://muxuezi.github.io/posts/optimizing-the-ridge-regression-parameter.html</link><dc:creator>Tao Junjie</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="优化岭回归参数"&gt;优化岭回归参数&lt;a class="anchor-link" href="https://muxuezi.github.io/posts/optimizing-the-ridge-regression-parameter.html#%E4%BC%98%E5%8C%96%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%8F%82%E6%95%B0"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;当你使用岭回归模型进行建模时，需要考虑&lt;code&gt;Ridge&lt;/code&gt;的&lt;code&gt;alpha&lt;/code&gt;参数。&lt;/p&gt;
&lt;p&gt;例如，用OLS（普通最小二乘法）做回归也许可以显示两个变量之间的某些关系；但是，当&lt;code&gt;alpha&lt;/code&gt;参数正则化之后，那些关系就会消失。做决策时，这些关系是否需要考虑就显得很重要了。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/optimizing-the-ridge-regression-parameter.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><category>scikit-learn cookbook</category><guid>https://muxuezi.github.io/posts/optimizing-the-ridge-regression-parameter.html</guid><pubDate>Tue, 18 Aug 2015 04:57:47 GMT</pubDate></item><item><title>using-ridge-regression-to-overcome-linear-regression-shortfalls</title><link>https://muxuezi.github.io/posts/using-ridge-regression-to-overcome-linear-regression-shortfalls.html</link><dc:creator>Tao Junjie</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="用岭回归弥补线性回归的不足"&gt;用岭回归弥补线性回归的不足&lt;a class="anchor-link" href="https://muxuezi.github.io/posts/using-ridge-regression-to-overcome-linear-regression-shortfalls.html#%E7%94%A8%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%BC%A5%E8%A1%A5%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%B8%8D%E8%B6%B3"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;本主题将介绍岭回归。和线性回归不同，它引入了正则化参数来“缩减”相关系数。当数据集中存在共线因素时，岭回归会很有用。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/using-ridge-regression-to-overcome-linear-regression-shortfalls.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><category>scikit-learn cookbook</category><guid>https://muxuezi.github.io/posts/using-ridge-regression-to-overcome-linear-regression-shortfalls.html</guid><pubDate>Tue, 18 Aug 2015 04:57:47 GMT</pubDate></item><item><title>directly-applying-bayesian-ridge-regression</title><link>https://muxuezi.github.io/posts/directly-applying-bayesian-ridge-regression.html</link><dc:creator>Tao Junjie</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="贝叶斯岭回归"&gt;贝叶斯岭回归&lt;a class="anchor-link" href="https://muxuezi.github.io/posts/directly-applying-bayesian-ridge-regression.html#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%B2%AD%E5%9B%9E%E5%BD%92"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;在&lt;em&gt;用岭回归弥补线性回归的不足&lt;/em&gt;主题中，我们介绍了岭回归优化的限制条件。我们还介绍了相关系数的先验概率分布的贝叶斯解释，将很大程度地影响着先验概率分布，先验概率分布通常均值是0。&lt;/p&gt;
&lt;p&gt;因此，现在我们就来演示如何scikit-learn来应用这种解释。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/directly-applying-bayesian-ridge-regression.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><category>scikit-learn cookbook</category><guid>https://muxuezi.github.io/posts/directly-applying-bayesian-ridge-regression.html</guid><pubDate>Tue, 18 Aug 2015 04:57:47 GMT</pubDate></item><item><title>using-sparsity-to-regularize-models</title><link>https://muxuezi.github.io/posts/using-sparsity-to-regularize-models.html</link><dc:creator>Tao Junjie</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="LASSO正则化"&gt;LASSO正则化&lt;a class="anchor-link" href="https://muxuezi.github.io/posts/using-sparsity-to-regularize-models.html#LASSO%E6%AD%A3%E5%88%99%E5%8C%96"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;LASSO（ least absolute shrinkage and selection operator，最小绝对值收缩和选择算子）方法与岭回归和LARS（least angle regression，最小角回归）很类似。与岭回归类似，它也是通过增加惩罚函数来判断、消除特征间的共线性。与LARS相似的是它也可以用作参数选择，通常得出一个相关系数的稀疏向量。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/using-sparsity-to-regularize-models.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><category>scikit-learn cookbook</category><guid>https://muxuezi.github.io/posts/using-sparsity-to-regularize-models.html</guid><pubDate>Tue, 18 Aug 2015 04:57:47 GMT</pubDate></item><item><title>fitting-a-line-through-data</title><link>https://muxuezi.github.io/posts/fitting-a-line-through-data.html</link><dc:creator>Tao Junjie</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="线性回归模型"&gt;线性回归模型&lt;a class="anchor-link" href="https://muxuezi.github.io/posts/fitting-a-line-through-data.html#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;现在，我们来做一些建模！我们从最简单的线性回归（Linear regression）开始。线性回归是最早的也是最基本的模型——把数据拟合成一条直线。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/fitting-a-line-through-data.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><category>scikit-learn cookbook</category><guid>https://muxuezi.github.io/posts/fitting-a-line-through-data.html</guid><pubDate>Tue, 18 Aug 2015 04:57:47 GMT</pubDate></item><item><title>taking-a-more-fundamental-approach-to-regularization-with-lars</title><link>https://muxuezi.github.io/posts/taking-a-more-fundamental-approach-to-regularization-with-lars.html</link><dc:creator>Tao Junjie</dc:creator><description>&lt;div tabindex="-1" id="notebook" class="border-box-sizing"&gt;
    &lt;div class="container" id="notebook-container"&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="LARS正则化"&gt;LARS正则化&lt;a class="anchor-link" href="https://muxuezi.github.io/posts/taking-a-more-fundamental-approach-to-regularization-with-lars.html#LARS%E6%AD%A3%E5%88%99%E5%8C%96"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;如果斯坦福大学的Bradley Efron, Trevor Hastie, Iain Johnstone和Robert Tibshirani没有发现它的话[1]，LARS(Least Angle Regression，最小角回归)可能有一天会被你想出来，它借用了&lt;a href="https://en.wikipedia.org/wiki/Gilbert_Strang"&gt;威廉·吉尔伯特·斯特朗（William Gilbert Strang）&lt;/a&gt;介绍过的高斯消元法（Gaussian elimination）的灵感。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/taking-a-more-fundamental-approach-to-regularization-with-lars.html"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><category>scikit-learn cookbook</category><guid>https://muxuezi.github.io/posts/taking-a-more-fundamental-approach-to-regularization-with-lars.html</guid><pubDate>Tue, 18 Aug 2015 04:57:47 GMT</pubDate></item></channel></rss>